import os
from pprint import pprint
configfile: "config.yaml"

# we are going to make some top level directories under Results so that errors do not occur
os.makedirs(config['results_folder'] + "Alignments/", exist_ok=True)
os.makedirs(config['results_folder'] + "guppy/", exist_ok=True)
os.makedirs(config['results_folder'] + "vsearch/", exist_ok=True)
os.makedirs(config['results_folder'] + "minimap/", exist_ok=True)
os.makedirs(config['results_folder'] + "Barcode/", exist_ok=True)
os.makedirs(config['results_folder'] + "Basecall/", exist_ok=True)
os.makedirs(config['results_folder'] + "CountReads/", exist_ok=True)
os.makedirs(config['results_folder'] + "DataFiles/", exist_ok=True)
os.makedirs(config['results_folder'] + "NanoFilt/", exist_ok=True)
os.makedirs(config['results_folder'] + "NanoQC/", exist_ok=True)
os.makedirs(config['results_folder'] + "Trim/", exist_ok=True)
os.makedirs(config['results_folder'] + "Visuals/", exist_ok=True)

def ReturnBarcodeTextFileNames():
    folder_numbers = list()
    directory_path = config['results_folder'] + "Basecall"
    directories = os.listdir(directory_path)
    for item in directories:

    # we want to exclude any 'hidden' directories, as the directories we are interested in do not start with '.'
        if item[0] is not ".":
            folder_number = item.split("_")[-1]
            folder_numbers.append(folder_number)
    return folder_numbers

# Wildcard section; this is how file names will be retrieved
# commas are added after the file name to "unpack" the tuple that is generated
BASECALL_SAMPLES, = glob_wildcards(config['results_folder'] + "DataFiles/fast5/{fast5_file}.fast5")
BARCODE_FILE_NUMBER = ReturnBarcodeTextFileNames()
TRIM_SAMPLES, = glob_wildcards(config['results_folder'] + "Barcode/{fastq_file}.fastq")
NANOQC_SAMPLES, = glob_wildcards(config['results_folder'] + "Trim/{fastq_file}.fastq")
POST_TRIM_FOLDER_NAMES = os.listdir(config['results_folder'] + "Trim")



rule all:
    input:
        # basecall
        expand(config['results_folder'] + "Basecall/{fast5_file}", fast5_file=BASECALL_SAMPLES),

        # barcode
        expand(config['results_folder'] + "Barcode/basecall_id_{run_id}_delete_to_run_again", run_id=BARCODE_FILE_NUMBER),

        # merge files
        config['results_folder'] + "Barcode/merge_files_delete_this_to_merge_again",

        # trim reads
        expand(config['results_folder'] + "Trim/{fastq_file}.trimmed.fastq", fastq_file=TRIM_SAMPLES),

        # NanoQC
        expand(config['results_folder'] + "NanoQC/{barcode_folder}/{barcode_folder}.nanoqc.log", barcode_folder=POST_TRIM_FOLDER_NAMES),
        expand(config['results_folder'] + "NanoQC/{barcode_folder}/{barcode_folder}.nanoqc.html", barcode_folder=POST_TRIM_FOLDER_NAMES),

        # NanoFilt
        expand(config['results_folder'] + "NanoFilt/{barcode_folder}/{barcode_folder}.nanofilt.fastq", barcode_folder=POST_TRIM_FOLDER_NAMES),

        # guppy aligner
        expand(config['results_folder'] + "Alignments/guppy/alignment_summary/{barcode_number}.guppy.alignment_summary.csv", barcode_number=POST_TRIM_FOLDER_NAMES),
        expand(config['results_folder'] + "Alignments/guppy/logs/{barcode_number}.guppy.log", barcode_number=POST_TRIM_FOLDER_NAMES),
        expand(config['results_folder'] + "Alignments/guppy/sam_files/{barcode_number}.guppy.sam", barcode_number=POST_TRIM_FOLDER_NAMES),

        # create simple statistics
        expand(config['results_folder'] + "Alignments/guppy/simple_statistics/{barcode_number}.guppy.simple_statistics.csv", barcode_number=POST_TRIM_FOLDER_NAMES),

        # create collated simple statistics
        config['results_folder'] + "Alignments/guppy/collated_simple_statistics.csv"


rule basecall:
    input:
        config['results_folder'] + "DataFiles/fast5/{fast5_file}.fast5"
    output:
        directory(config['results_folder'] + "Basecall/{fast5_file}")
    conda:
        "envs/pythonEnv.yaml"
    shell:
        "ls {input}"

        " | "  # pipe results from `ls {input}` into guppy_basecaller

        # we are not giving guppy an input path. It will assume the input from the `ls` command
        "guppy_basecaller "
        "--quiet "
        "--save_path {output} "
        "--config dna_r9.4.1_450bps_fast.cfg "
        "--num_callers 1 "
        "--cpu_threads_per_caller 2 "

        " && "

        "echo BASECALL FILE OUTPUT: {output}"


"""
Merging files after barcoding comes with a challenge
Snakemake will see the missing `output` files, and think that barcoding needs to be ran again
To overcome this, we will `touch` an output file when this process is done
This allows us to move the real output files around (with `rule merge_files`), without having
    to redo barcoding every time because snakemake thinks it has not been ran yet

To get around the issue of showing the number of barcodes that need to be ran, we will create a file

    by placing a period in front of a text file, but keep our actual output directories as a parameter
This allows us to still manipulate the output in `rule merge_files`, while still letting snakemake know that
    the barcodes do not need to be run
"""
rule barcode:
    input:
        config['results_folder'] + "Basecall/"
    output:
        touch(config['results_folder'] + "Barcode/basecall_id_{run_id}_delete_to_run_again")
    params:
        real_output = config['results_folder'] + "Barcode/"
    conda:
        "envs/pythonEnv.yaml"
    shell:
        "guppy_barcoder "
        "--input_path {input} "
        "--save_path {params.real_output} "
        "--recursive "
        "--quiet "
        "--config configuration.cfg "
        "--worker_threads 2 "
        "--barcode_kits EXP-PBC096 "
        "--require_barcodes_both_ends "

        " && "

        "echo BARCODE FILE OUTPUT: {params.real_output}"

"""
As in the `rule barcode`, a similar practice is happening here
It is not possible to use the same input and output within a rule in snakemake, otherwise a cyclic dependency occurs
To get around this, we are going to simply create a file when this rule is complete, indicating that merging has
    occurred
This rule will not show the total number of files that need to be concatenated, as this process is very fast
We are also using the output from `rule barcode` as input here so this rule does not complete before barcoding is done
"""
rule merge_files:
    input:
        barcode_folders = config['results_folder'] + "Barcode",
        barcode_output_status = expand(config['results_folder'] + "Barcode/basecall_id_{run_id}_delete_to_run_again", run_id=BARCODE_FILE_NUMBER)
    output:
        touch(config['results_folder'] + "Barcode/merge_files_delete_this_to_merge_again")
    run:
        # import libraries needed to find files, search for names, and move files
        import os
        import shutil
        import re

        # get our parent folder so we don't have to type `rules.merge_files.input` every time
        parent_folder = str(rules.merge_files.input.barcode_folders)

        # iterate through each directory in our parent folder
        for root, directories, files in os.walk(parent_folder):
            for directory in directories:

                # get the number of files in the directory
                current_dir_path = os.path.join(root, directory)
                number_of_files = len(os.listdir(current_dir_path))

                # get the old file path
                old_file_path = current_dir_path + "/" + str(os.listdir(current_dir_path)[0])

                # find its barcode number (to set a new name)
                barcode_number = re.search("(barcode)[0-9]{2}", old_file_path)
                if barcode_number is None:
                    barcode_number = re.search("(unclassified)", old_file_path)
                barcode_number = old_file_path[barcode_number.start():barcode_number.end()]

                # create our new file path
                new_file_path = current_dir_path + "/{0}.merged.fastq".format(barcode_number)

                # we only need to rename the file (technically move it) from the old path to the new path
                # both paths are in the same folder, just different names
                if number_of_files < 2:

                    # move the file to its new location
                    shutil.copy2(old_file_path, new_file_path)
                    os.remove(old_file_path)

                # we need to concatenate if there are 2 or more files in the directory
                else:
                    # first get the files we need to concatenate
                    files_to_concatenate = os.listdir(current_dir_path)

                    try:
                        open(new_file_path, 'r').close()
                    # we did not find the file, create one instead
                    except FileNotFoundError:
                        open(new_file_path, 'w').close()

                    # now open the new file as output_stream to write data
                    with open(new_file_path, 'a') as output_stream:

                        # iterate through each of our files to concatenate
                        for file in files_to_concatenate:

                            # create our input file path
                            input_file_path = current_dir_path + "/" + file

                            # now open our input file path as input_stream so we can read data
                            with open(input_file_path, 'r') as input_stream:

                                # iterate through each line in the file and write it to our new file
                                for each_line in input_stream:
                                    output_stream.write(each_line)

                    # we can now delete the old files
                    for file in files_to_concatenate:
                        delete_file_path = current_dir_path + "/" + file
                        os.remove(delete_file_path)

rule trim_reads:
    input:
        config['results_folder'] + "Barcode/{barcode_number}.fastq"
    output:
        config['results_folder'] + "Trim/{barcode_number}.trimmed.fastq"
    params:
        three_prime_adapter = "ACTTGCCTGTCGCTCTATCTTCTACCTTGTTACGACTT",
        five_prime_adapter = "TTTCTGTTGGTGCTGATATTGCAGRGTTYGATYMTGGCTCAG",
        error_rate = 0.15
    shell:
        "cutadapt "
        "--revcomp "
        "--quiet "
        "--cores 0 "  # auto assign cores
        "--adapter {params.three_prime_adapter} "
        "--front {params.five_prime_adapter} "
        "--error-rate {params.error_rate} "
        "--output {output} "
        "{input}"

rule NanoQC:
    input:
        config['results_folder'] + "Trim/{barcode_folder}/{barcode_folder}.merged.trimmed.fastq"
    output:
        log_file = config['results_folder'] + "NanoQC/{barcode_folder}/{barcode_folder}.nanoqc.log",
        html_file = config['results_folder'] + "NanoQC/{barcode_folder}/{barcode_folder}.nanoqc.html"
    params:
        output_dir = config['results_folder'] + "NanoQC/{barcode_folder}",
        barcode_folder = "{barcode_folder}"
    shell:
        "nanoqc "
        "-o {params.output_dir} "
        "{input} "

        " && "

        "mv {params.output_dir}/nanoQC.html {output.html_file}"
        " && "
        "mv {params.output_dir}/NanoQC.log {output.log_file}"

rule NanoFilt:
    input:
        config['results_folder'] + "Trim/{barcode_folder}/{barcode_folder}.merged.trimmed.fastq"
    output:
        config['results_folder'] + "NanoFilt/{barcode_folder}/{barcode_folder}.nanofilt.fastq"
    shell:
        "nanofilt {input} > {output}"

"""
Much like guppy_barcoder, guppy_aligner prefers to use folders as input as well. We are going to use a direct copy from NanoFilt's output to this input
    so snakemake knows that it must wait for NanoFilt to be finished before it can run guppy_aligner
However, for our output, we are going to use a temporary input and output path so that each instance of guppy_aligner that is created is only given one file,
    as opposed to the entire NanoFilt directory. This will ensure that only one instance of the guppy_aligner rule processes one file.
We will then move our outputs after each instance of the rule is run, and delete the temporary output folder (.temp_output/barcode##)
"""
rule guppy_aligner:
    input:
        config['results_folder'] + "NanoFilt/{barcode_number}/{barcode_number}.nanofilt.fastq"
    output:
        alignment_summary = config['results_folder'] + "Alignments/guppy/alignment_summary/{barcode_number}.guppy.alignment_summary.csv",
        log_files = config['results_folder'] + "Alignments/guppy/logs/{barcode_number}.guppy.log",
        sam_files = config['results_folder'] + "Alignments/guppy/sam_files/{barcode_number}.guppy.sam",
        temp_output = temp(directory(config['results_folder'] + "Alignments/guppy/.temp_output/{barcode_number}")),
    params:
        temp_input = config['results_folder'] + "NanoFilt/{barcode_number}",
        alignment_reference_location = config['alignment_reference_file'],
        barcode_number = '{barcode_number}',
        simple_statistics = config['results_folder'] + "Alignments/guppy/simple_statistics/{barcode_number}.guppy.simple_statistics.csv"
    shell:
        "rm -rf {output.temp_output}"  # remove current directories otherwise guppy is upset

        " && "

        # run guppy aligner with the following parameters
        "guppy_aligner "
        "--quiet "
        "--input_path {params.temp_input} "
        "--save_path {output.temp_output} "
        "--align_ref {params.alignment_reference_location} "

        " && "

        # move output files from .temp_output to their respective folders
        "mv {output.temp_output}/*.txt {output.alignment_summary}"
        " && "
        "mv {output.temp_output}/*.sam {output.sam_files}"
        " && "
        "mv {output.temp_output}/*.log {output.log_files}"
        " && "
        "touch {params.simple_statistics}"

rule guppy_write_simple_statistics:
    input:
        input_file = config['results_folder'] + "Alignments/guppy/alignment_summary/{barcode_number}.guppy.alignment_summary.csv"
    output:
        statistics_path = config['results_folder'] + "Alignments/guppy/simple_statistics/{barcode_number}.guppy.simple_statistics.csv"
    params:
        file_path = config['results_folder'] + "Alignments/guppy/alignment_summary/{barcode_number}.guppy.alignment_summary.csv"
    run:
        import csv
        import pandas as pd

        id_classified = 0
        id_unclassified = 0

        input_file = input.input_file

        data_frame = pd.read_csv(filepath_or_buffer=input_file,
                                 sep="\t",
                                 header=0,
                                 dtype={
                                     "read_id": str(),
                                     "alignment_genome": str(),
                                     "alignment_genome_start": int,
                                     "alignment_genome_end": int,
                                     "alignment_strand_start": int,
                                     "alignment_strand_end": int,
                                     "alignment_num_insertions": int,
                                     "alignment_num_deletions": int,
                                     "alignment_num_aligned": int,
                                     "alignment_num_correct": int,
                                     "alignment_identity": float,
                                     "alignment_accuracy": float,
                                     "alignment_score": int})

        for row in data_frame['alignment_genome']:
            if row == "*":
                id_unclassified += 1
            else:
                id_classified += 1

        barcode_reads = id_classified + id_unclassified
        percent_classified = "{:.3f}".format( (id_classified / barcode_reads) * 100 )
        percent_unclassified = "{:.3f}".format( (id_unclassified / barcode_reads) * 100 )

        row_one = ["barcode_reads", "classified_reads", "unclassified_reads", "percent_classified", "percent_unclassified"]
        row_two = [barcode_reads, id_classified, id_unclassified, percent_classified, percent_unclassified]

        with open(output.statistics_path, 'w') as output_stream:
            csv_writer = csv.writer(output_stream, delimiter="\t")
            csv_writer.writerow(row_one)
            csv_writer.writerow(row_two)

"""
This rule will iterate over the simple statistics csv files found under ARS/Results/Alignments/guppy/simple_statistics and collate their results into
    one master statistics file. The same statistics are going to be found (total barcodes, classified, unclassified, percent classified, pecent unclassified)
"""
rule guppy_aligner_collated_simple_statistics:
    input:
        files = [expand(config['results_folder'] + "Alignments/guppy/simple_statistics/{barcode_number}.guppy.simple_statistics.csv", barcode_number=POST_TRIM_FOLDER_NAMES)]
    output:
        save_file = config['results_folder'] + "Alignments/guppy/collated_simple_statistics.csv"
    run:
        import csv
        import pandas as pd

        # we have to split our input because snakemake is passing the input as a string, not a list
        input_files = str(input.files).split(" ")

        barcode_reads = 0
        classified_reads = 0
        unclassified_reads = 0

        for file in input_files:
            data_frame = pd.read_csv(filepath_or_buffer=file,
                                     sep="\t",
                                     header=0,
                                     dtype={
                                         "barcode_reads":int,
                                         "classified_reads":int,
                                         "unclassified_reads":int,
                                         "percent_classified":float,
                                         "percent_unclassified":float
                                     })
            barcode_reads += data_frame['barcode_reads']
            classified_reads += data_frame['classified_reads']
            unclassified_reads += data_frame['unclassified_reads']

        percent_classified = (classified_reads / barcode_reads) * 100
        percent_unclassified = (unclassified_reads / barcode_reads) * 100

        row_one = ["barcode_reads", "classified_reads", "unclassified_reads", "percent_classified", "percent_unclassified"]
        row_two = [
            int(barcode_reads),
            int(classified_reads),
            int(unclassified_reads),
            "{:.3f}".format(float(percent_classified)),
            "{:.3f}".format(float(percent_unclassified))]

        with open(output.save_file, 'w') as output_stream:
            csv_writer = csv.writer(output_stream, delimiter="\t")
            csv_writer.writerow(row_one)
            csv_writer.writerow(row_two)